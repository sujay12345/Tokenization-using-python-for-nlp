Tokenization is the basics of Natural Language Processing. Tokenize all the words in german_text using word_tokenize(), and print the result. Tokenize only the capital words in german_text. First, write a pattern called capital_words to match only capital words. Then, tokenize it using regexp_tokenize(). Tokenize only the emoji in german_text.
